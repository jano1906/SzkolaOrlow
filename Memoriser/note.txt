Using classical models but reducing time and space of runtime complexity by storing a look up table
of grid of points. Searching for a cube in O(d) and then computing affine interpolation.
Binary classification problems might be encoded using n bits of data. Therefore cloud of milion points may be stored in 1Mb. We can also try using compression algoritms.

3 steps:
-> train model
-> find distribution of data (clustering + center of mass + gaussian ?)
-> make synthetic dataset of points lying on exact coordinates 
	(a,b) -T-> T(a,b) -N-> N(T(a,b))
-> use kernel trick (?) and affine interpolation to predict future observations 
	(a,b) -(Tinv)-> Tinv(a,b) -A-> A(Tinv(a,b))

additional: compression algorithms for lookup table

todo:
- implement full Affine Interpolation algoritm
- test loss of accuracy and compare models (KNN, AI, RF) with their encoded version
- Manifold learning and dimension reduction
- Test and see where different answers are given using encoded and non-encoded model
- Reconstruction of cdf and ppf from cloud of datapoints
- Try using encoding for regression algorithms
- Real use case: data generated by gym
- Try using Kernel trick
- How encoding smoothens the data, how it helps when data is noisy
- Compare the algoritm to other look up techniques in KNN, d-trees etc...
